<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="hFGM4TxPxthFmk31fTMEoGDKPU69WmAzgBZsOC0bzAk"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yong Zheng-Xin </title> <meta name="author" content="Yong Zheng-Xin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yongzx.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%63%6F%6E%74%61%63%74.%79%6F%6E%67@%62%72%6F%77%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=z6bOk-AAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/yong_zhengxin" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"><a class="nav-link" href="https://newsletter.yongzx.io/" rel="external nofollow noopener" target="_blank">blog</a></li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yong</span> Zheng-Xin </h1> <p class="desc"><span style="color:grey">Current: Computer Science Ph.D. @ Brown University<br>Past: Research Scientist Intern @ <a href="https://ai.meta.com/" style="color:#222222" rel="external nofollow noopener" target="_blank">Meta AI</a>, Research Collaborator @ <a href="https://cohere.com/research" style="color:#222222" rel="external nofollow noopener" target="_blank">Cohere For AI</a></span></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?65b8945f98879ec84f71ac14c8bc7b73" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic.jpg" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <style type="text/css">.tab{margin-left:30px}</style> <p>I am a fourth-year Ph.D. student in Computer Science at Brown University, advised by <a href="https://scholar.google.com/citations?user=hs6pGXoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Prof. Stephen Bach</a>. I investigate fundamental properties of large language models, particularly how they acquire and transfer capabilities across languages, domains, and tasks. I am fortunate to have worked with researchers at <u>Meta GenAI</u> (with <a href="https://jfchi.github.io/" rel="external nofollow noopener" target="_blank">Jianfeng Chi</a>), <u>Meta AI FAIR</u> (with <a href="https://maillard.it/" rel="external nofollow noopener" target="_blank">Jean Maillard</a> and <a href="https://michaelauli.github.io/" rel="external nofollow noopener" target="_blank">Michael Auli</a>), and <u>Cohere Labs</u> (with <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, <a href="https://scholar.google.com/citations?user=v2cMiCAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Beyza Ermis</a>, <a href="https://marziehf.github.io/" rel="external nofollow noopener" target="_blank">Marzieh Fadaee</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>).</p> <p>I work on generalization of post-training to make models more capable and safe, often using cross-language phenomena as scientific lens to reveal core principles of how LLMs generalize to out-of-distribution settings. I also spend time work on multilingual foundational models and thinking about their social outcomes. My research interests and past work include:</p> <ul> <li> <p><strong>Generalization of test-time thinking.</strong> My most recent work reveals fundamental properties of test-time scaling by using cross-language and cross-domain transfers to study generalization boundaries. We observed robust crosslingual generalization of English reasoning finetuning through the “quote-and-think” pattern, while finding limited cross-domain transfer from math domains <a href="https://arxiv.org/abs/2505.05408" rel="external nofollow noopener" target="_blank">(preprint)</a>.</p> </li> <li> <p><strong>Generalization of safety alignment.</strong> I discovered low-resource languages can jailbreak GPT-4 <a href="https://arxiv.org/abs/2310.02446" rel="external nofollow noopener" target="_blank">(⭑Best Paper Award, NeurIPS 2023 Socially Responsible Language Modeling Workshop)</a>, revealing core limitations in how alignment training interacts with multilingual representations. This work became the seminal work for multilingual red-teaming and has shaped safety frameworks at major AI developers including <a href="https://cdn.openai.com/gpt-4o-system-card.pdf" rel="external nofollow noopener" target="_blank">OpenAI</a>, <a href="https://arxiv.org/abs/2407.21783" rel="external nofollow noopener" target="_blank">Meta</a>, and <a href="https://arxiv.org/abs/2407.13833" rel="external nofollow noopener" target="_blank">Microsoft</a>. The work was also highlighted in the <a href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai" rel="external nofollow noopener" target="_blank">first International Scientific Report on the Safety of Advanced AI (2024)</a> and featured on <a href="https://www.newscientist.com/article/2398656-gpt-4-gave-advice-on-planning-terrorist-attacks-when-asked-in-zulu/" rel="external nofollow noopener" target="_blank">New Scientist</a>. I have since worked on the mechanistic interpretability of crosslingual detoxification <a href="https://arxiv.org/abs/2406.16235" rel="external nofollow noopener" target="_blank">(EMNLP 2024 Findings)</a> and crosslingual finetuning attacks <a href="https://arxiv.org/abs/2410.18210" rel="external nofollow noopener" target="_blank">(NAACL 2025 Findings)</a>.</p> </li> <li> <p><strong>Generalization of instruction-following.</strong> I was the co-first author of the open-source Aya model <a href="https://arxiv.org/abs/2402.07827" rel="external nofollow noopener" target="_blank">(⭑Best Paper Award, ACL 2024)</a> and a core contributor to foundational instruction-following models T0 (<a href="https://arxiv.org/abs/2110.08207" rel="external nofollow noopener" target="_blank">ICLR 2022 Spotlight</a>, <a href="https://arxiv.org/abs/2202.01279" rel="external nofollow noopener" target="_blank">ACL 2022 Demo</a>) and mT0 <a href="https://arxiv.org/abs/2110.08207" rel="external nofollow noopener" target="_blank">(ACL 2023)</a>. These work demonstrated that models can learn generalizable instruction-following patterns that transfer across linguistic boundaries, establishing foundational principles for multilingual instruction tuning at scale.</p> </li> <li> <p><strong>Efficient adaptation to unseen languages and speech accents</strong>: I investigate how pretrained models can efficiently generalize to unseen languages through language adaptation (<a href="https://arxiv.org/abs/2212.09535" rel="external nofollow noopener" target="_blank">ACL 2023</a>) and synthetic data <a href="https://arxiv.org/abs/2402.14086" rel="external nofollow noopener" target="_blank">(EMNLP 2024 Findings)</a>, contributing to mid-training methodologies. I also studied speech pattern distributions across accent groups to understand generalization principles to unseen accents in automatic speech recognition (INTERSPEECH 2025).</p> </li> </ul> <p>Other Misc Stuff:</p> <ul> <li>I am a Malaysian 🇲🇾 and I contributed substantially to NLP for South-East Asian (SEA) languages. For instance, I studied language-mixing behaviors for SEA languages (<a href="https://arxiv.org/abs/2303.13592" rel="external nofollow noopener" target="_blank">EMNLP 2023 CALCS</a>, <a href="https://aclanthology.org/2023.findings-emnlp.382/" rel="external nofollow noopener" target="_blank">EMNLP 2023 Findings</a>, <a href="https://aclanthology.org/2023.findings-acl.185/" rel="external nofollow noopener" target="_blank">ACL 2023 Findings</a>), contributed Malaysian cultural data to <a href="https://arxiv.org/abs/2406.10118" rel="external nofollow noopener" target="_blank">SEACrowd</a> and <a href="https://arxiv.org/abs/2406.05967" rel="external nofollow noopener" target="_blank">CVQA</a>, and co-hosted a tutorial for SEA NLP (<a href="https://aclanthology.org/2023.ijcnlp-tutorials.2/" rel="external nofollow noopener" target="_blank">AACL 2023</a>).</li> <li>I went to <a href="https://www.minerva.edu/" rel="external nofollow noopener" target="_blank">Minerva University</a> during my undergrad so I had the opportunity to travel and live in six different countries for at least four months: United States (San Francisco), South Korea (Seoul), India (Hyderabad), Germany (Berlin), Argentina (Buenos Aires), and United Kingdom (London).</li> <li>One of my biggest passion outside of work is dancing 🕺, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. I usually check out the dance scenes in the city when I travel to conferences ––– if you also enjoy dancing, hmu we can check them out together.</li> </ul> </div> <hr> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2025</span></th> <td> <span style="color:black">Gave an invited talk at <a href="https://milanlproc.github.io/#about" rel="external nofollow noopener" target="_blank">MilaNLP lab</a>. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2025</span></th> <td> <span style="color:black"><strong>1 paper accepted!</strong> Work on mitigating accent bias in ASR was accepted to INTERSPEECH’25. <br>Work was done during Meta internship. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">02 / 2025</span></th> <td> <span style="color:black"><strong>1 paper accepted!</strong> Work on <a href="https://arxiv.org/abs/2410.18210" rel="external nofollow noopener" target="_blank">cross-lingual finetuning attacks</a> was accepted to NAACL’25 findings.<br> Work was done during Meta internship. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">09 / 2024</span></th> <td> <span style="color:black"><strong>4 papers accepted!</strong> <a href="https://arxiv.org/abs/2402.14086" rel="external nofollow noopener" target="_blank">LexC-Gen</a>, <a href="https://arxiv.org/abs/2406.10118" rel="external nofollow noopener" target="_blank">SEACrowd</a>, and <a href="https://arxiv.org/abs/2406.16235" rel="external nofollow noopener" target="_blank">crosslingual alignment</a> were accepted to EMNLP. <a href="https://arxiv.org/abs/2406.05967" rel="external nofollow noopener" target="_blank">CVQA</a> was accepted to NeurIPS. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">08 / 2024</span></th> <td> <span style="color:black"><strong><a href="https://arxiv.org/abs/2402.07827" rel="external nofollow noopener" target="_blank"><span style="color:brown;">Aya Model paper</span></a></strong> received the <strong>⭑Best Paper Award</strong> at ACL 2024. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">07 / 2024</span></th> <td> <span style="color:black">Gave an invited talk at <a href="https://lu.ma/fl2t9xms" rel="external nofollow noopener" target="_blank">London Data Week</a>. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">06 / 2024</span></th> <td> <span style="color:black">Started research scientist internship at Meta AI (FAIR)! </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2024</span></th> <td> <span style="color:black"><strong>1 paper accepted!</strong> <a href="https://arxiv.org/abs/2403.04893" rel="external nofollow noopener" target="_blank">A Safe Harbor for AI Evaluation and Red Teaming</a> is accepted to ICML. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">02 / 2024</span></th> <td> <span style="color:black">Released <a href="https://cohere.com/research/aya" rel="external nofollow noopener" target="_blank">Aya</a> model and dataset papers! <br>I also presented Aya multilingual safety research at Aya Grand Finale. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">11 / 2023</span></th> <td> <span style="color:black">Co-organized the <a href="https://aacl2023-sea-nlp.github.io/" rel="external nofollow noopener" target="_blank">tutorial on current status of NLP in South East Asia</a> at AACL 2023. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">10 / 2023</span></th> <td> <span style="color:black">“<strong><a href="https://arxiv.org/abs/2310.02446" rel="external nofollow noopener" target="_blank"><span style="color:brown;">Low-Resource Languages Jailbreak GPT-4</span></a></strong>” received the <strong>⭑Best Paper Award</strong> at NeurIPS 2023 Socially Responsible Language Modeling (SoLaR) workshop. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">09 / 2023</span></th> <td> <span style="color:black">Joined the Cohere For AI’s Responsible Deployment Team for <a href="https://sites.google.com/cohere.com/aya-en/home" rel="external nofollow noopener" target="_blank">Aya</a> red-teaming. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">08 / 2023</span></th> <td> <span style="color:black">Served as the Area Chair (Multilingualism &amp; Linguistic Diversity Track in <a href="https://2023.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2023</a>). </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2023</span></th> <td> <span style="color:black"><strong>Media</strong>: Our <a href="https://arxiv.org/abs/2303.13592" rel="external nofollow noopener" target="_blank">code-switching paper</a> was featured by <a href="https://www.wired.com/story/chatgpt-non-english-languages-ai-revolution/" rel="external nofollow noopener" target="_blank">Wired</a>. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2023</span></th> <td> <span style="color:black"><strong>3 papers accepted!</strong> <a href="https://arxiv.org/abs/2212.09535" rel="external nofollow noopener" target="_blank">BLOOM+1</a>, <a href="https://arxiv.org/abs/2211.01786" rel="external nofollow noopener" target="_blank">BLOOMZ</a> and <a href="https://arxiv.org/abs/2212.09660" rel="external nofollow noopener" target="_blank">code-switching survey</a> were accepted to ACL 2023. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">03 / 2022</span></th> <td> <span style="color:black"><strong>2 papers accepted!</strong> <a href="https://arxiv.org/abs/2110.08207" rel="external nofollow noopener" target="_blank">T0</a> was accepted to ICLR (Spotlight). <a href="https://arxiv.org/abs/2202.01279" rel="external nofollow noopener" target="_blank">PromptSource</a> was accepted to ACL Demo. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">06 / 2021</span></th> <td> <span style="color:black">Started PhD at Brown University. </span> </td> </tr> </table> </div> </div> <hr> <h2> selected publications (<a href="/publications/" style="color: inherit">see all</a>) </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="yong2025safetysurvey" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2505.24119" target="_blank" style="color: black;" rel="external nofollow noopener">The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,  Beyza Ermis ,  Marzieh Fadaee , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Stephen H. Bach, Julia Kreutzer' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arxiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2505.24119" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020–2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong2025crosslingualtesttimescaling" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2505.05408" target="_blank" style="color: black;" rel="external nofollow noopener">Crosslingual Reasoning through Test-Time Scaling</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,  M. Farid Adilazuarda ,  Jonibek Mansurov , and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen H. Bach, Alham Fikri Aji' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>arxiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2505.05408" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/BatsResearch/crosslingual-test-time-scaling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM’s CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong2025asr" class="col-sm-11"> <div class="title"> Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,  Vineel Pratap ,  Michael Auli , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jean Maillard' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>INTERSPEECH</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>To build an automatic speech recognition (ASR) system that can serve everyone in the world, the ASR needs to be robust to a wide range of accents including unseen accents. We systematically study how three different variables in training data—the number of speakers, the audio duration per each individual speaker, and the diversity of accents—affect ASR robustness towards unseen accents in a low-resource training regime. We observe that for a fixed number of ASR training hours, it is more beneficial to increase the number of speakers (which means each speaker contributes less) than the number of hours contributed per speaker. We also observe that more speakers enables ASR performance gains from scaling number of hours. Surprisingly, we observe minimal benefits to prioritizing speakers with different accents when the number of speakers is controlled. Our work suggests that practitioners should prioritize increasing the speaker count in ASR training data composition for new languages</p> </div> </div> </div> </li> <li> <div class="row"> <div id="poppi2024towards" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2410.18210" target="_blank" style="color: black;" rel="external nofollow noopener">Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks</a> </div> <div class="author"> Samuele Poppi ,  <span style="font-weight:bold">Zheng-Xin Yong</span> ,  Yifei He , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Bobbie Chern, Han Zhao, Aobo Yang, Jianfeng Chi' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>NAACL Findings</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2410.18210" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>Recent advancements in Large Language Models (LLMs) have sparked widespread concerns about their safety. Recent work demonstrates that safety alignment of LLMs can be easily removed by fine-tuning with a few adversarially chosen instruction-following examples, i.e., fine-tuning attacks. We take a further step to understand fine-tuning attacks in multilingual LLMs. We first discover cross-lingual generalization of fine-tuning attacks: using a few adversarially chosen instruction-following examples in one language, multilingual LLMs can also be easily compromised (e.g., multilingual LLMs fail to refuse harmful prompts in other languages). Motivated by this finding, we hypothesize that safety-related information is language-agnostic and propose a new method termed Safety Information Localization (SIL) to identify the safety-related information in the model parameter space. Through SIL, we validate this hypothesis and find that only changing 20% of weight parameters in fine-tuning attacks can break safety alignment across all languages. Furthermore, we provide evidence to the alternative pathways hypothesis for why freezing safety-related parameters does not prevent fine-tuning attacks, and we demonstrate that our attack vector can still jailbreak LLMs adapted to new languages.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="li2024preference" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2406.16235" target="_blank" style="color: black;" rel="external nofollow noopener">Preference Tuning for Toxicity Mitigation Generalizes Across Languages</a> </div> <div class="author"> Xiaochen Li* ,  <span style="font-weight:bold">Zheng-Xin Yong*</span> ,  and  Stephen H Bach </div> <div class="periodical"> <em>EMNLP Findings</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2406.16235" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/BatsResearch/cross-lingual-detox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ustun2024aya" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2402.07827" target="_blank" style="color: black;" rel="external nofollow noopener">Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model</a> </div> <div class="author"> Ahmet Üstün* ,  Viraat Aryabumi* ,  <span style="font-weight:bold">Zheng-Xin Yong*</span> , and <span class="more-authors" title="click to view 14 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '14 more authors' ? 'Wei-Yin Ko*, Daniel D’souza*, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker' : '14 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">14 more authors</span> </div> <div class="periodical"> <em>ACL</em>, 2024 <span style="color:brown; font-weight:bold"> (Best Paper Award)</span> </div> <div class="periodical"> </div> <div> <span style="background-color: #FFEBB3;">Also featured in:</span> <a href="https://www.washingtonpost.com/politics/2024/02/13/ftcs-bedoya-says-laws-keep-teens-off-social-media-wont-work" rel="external nofollow noopener" target="_blank">The Washington Post</a> , <a href="https://www.theglobeandmail.com/business/article-ai-chatbots-fall-short-in-dozens-of-languages-a-non-profit-project" rel="external nofollow noopener" target="_blank">The Globe and Mail</a> , <a href="https://siliconangle.com/2024/02/13/cohere-ai-unveils-aya-multilingual-open-source-ai-101-languages/" rel="external nofollow noopener" target="_blank">SiliconANGLE</a>, etc. </div> <div class="links"> <a href="https://arxiv.org/abs/2402.07827" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages – including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong2023lowresource" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2310.02446" target="_blank" style="color: black;" rel="external nofollow noopener">Low-Resource Languages Jailbreak GPT-4</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,  Cristina Menghini ,  and  Stephen Bach </div> <div class="periodical"> <em>NeurIPS Workshop: Socially Responsible Language Modelling Research (SoLaR)</em> , 2023 <span style="color:brown; font-weight:bold"> (Best Paper Award)</span> </div> <div class="periodical"> </div> <div> <span style="background-color: #FFEBB3;">Also featured in:</span> <a href="https://www.newscientist.com/article/2398656-gpt-4-gave-advice-on-planning-terrorist-attacks-when-asked-in-zulu/" rel="external nofollow noopener" target="_blank">New Scientist</a> , <a href="https://hal.science/hal-04612963/document" rel="external nofollow noopener" target="_blank">UK AI Safety Institute (Scientific Report)</a> , <a href="https://www.zdnet.com/article/the-safety-of-openais-gpt-4-is-lost-in-translation/" rel="external nofollow noopener" target="_blank">ZDNet</a>, etc. </div> <div class="links"> <a href="https://arxiv.org/abs/2310.02446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4’s safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs’ safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong-etal-2023-prompting" class="col-sm-11"> <div class="title"> <a href="https://aclanthology.org/2023.calcs-1.5.pdf" target="_blank" style="color: black;" rel="external nofollow noopener">Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,  Ruochen Zhang ,  Jessica Forde , and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Skyler Wang, Arjun Subramonian, Holy Lovenia, Samuel Cahyawijaya, Genta Winata, Lintang Sutawika, Jan Christian Blaise Cruz, Yin Lin Tan, Long Phan, Long Phan, Rowena Garcia, Thamar Solorio, Alham Fikri Aji' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">13 more authors</span> </div> <div class="periodical"> <em>EMNLP Workshop: Computational Approaches to Linguistic Code-Switching (CALCS)</em> , 2023 </div> <div class="periodical"> </div> <div> <span style="background-color: #FFEBB3;">Also featured in:</span> <a href="https://www.wired.com/story/chatgpt-non-english-languages-ai-revolution/" rel="external nofollow noopener" target="_blank">WIRED</a> </div> <div class="links"> <a href="https://aclanthology.org/2023.calcs-1.5.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/Southeast-Asia-NLP/LLM-Code-Mixing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The differences in decision making between behavioural models of voice interfaces are hard to capture using existing measures for the absolute performance of such models. For instance, two models may have a similar task success rate, but very different ways of getting there. In this paper, we propose a general methodology to compute the similarity of two dialogue behaviour models and investigate different ways of computing scores on both the semantic and the textual level. Complementing absolute measures of performance, we test our scores on three different tasks and show the practical usability of the measures.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong-etal-2023-bloom" class="col-sm-11"> <div class="title"> <a href="https://aclanthology.org/2023.acl-long.653.pdf" target="_blank" style="color: black;" rel="external nofollow noopener">BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,  Hailey Schoelkopf ,  Niklas Muennighoff , and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, Vassilina Nikoulina' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">12 more authors</span> </div> <div class="periodical"> <em>ACL</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://aclanthology.org/2023.acl-long.653.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/bigscience-workshop/multilingual-modeling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at \urlhttps://github.com/bigscience-workshop/multilingual-modeling.</p> </div> </div> </div> </li> </ol> </div> <hr> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%6F%6E%74%61%63%74.%79%6F%6E%67@%62%72%6F%77%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=z6bOk-AAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/yong_zhengxin" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">Most active on X (Twitter). Also feel free to email me. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yong Zheng-Xin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Inspired by <a href="https://akariasai.github.io/" target="_blank" rel="external nofollow noopener">Akari Asai</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-K54RERLKHP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-K54RERLKHP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>