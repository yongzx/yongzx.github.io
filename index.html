<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="hFGM4TxPxthFmk31fTMEoGDKPU69WmAzgBZsOC0bzAk"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yong Zheng-Xin </title> <meta name="author" content="Yong Zheng-Xin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yongzx.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%63%6F%6E%74%61%63%74.%79%6F%6E%67@%62%72%6F%77%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=z6bOk-AAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/yong_zhengxin" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"><a class="nav-link" href="https://yongzx.notion.site/" rel="external nofollow noopener" target="_blank">blog</a></li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yong</span> Zheng-Xin </h1> <p class="desc"><span style="color:grey">Current: Computer Science Ph.D. @ Brown University<br>Past: Research Scientist Intern @ <a href="https://ai.meta.com/" style="color:#222222" rel="external nofollow noopener" target="_blank">Meta AI</a>, Research Collaborator @ <a href="https://cohere.com/research" style="color:#222222" rel="external nofollow noopener" target="_blank">Cohere For AI</a></span></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?65b8945f98879ec84f71ac14c8bc7b73" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic.jpg" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <style type="text/css">.tab{margin-left:30px}</style> <p>I am a fourth-year Ph.D. student in Computer Science at Brown University, advised by <a href="https://scholar.google.com/citations?user=hs6pGXoAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Prof. Stephen Bach</a>. I am fortunate to have worked with researchers at <u>Meta GenAI Safety Alignment</u> (with <a href="https://jfchi.github.io/" rel="external nofollow noopener" target="_blank">Jianfeng Chi</a>), <u>Meta AI FAIR</u> (with <a href="https://maillard.it/" rel="external nofollow noopener" target="_blank">Jean Maillard</a> and <a href="https://michaelauli.github.io/" rel="external nofollow noopener" target="_blank">Michael Auli</a>), and <u>Cohere Labs</u> (with <a href="https://juliakreutzer.github.io/" rel="external nofollow noopener" target="_blank">Julia Kreutzer</a>, <a href="https://scholar.google.com/citations?user=v2cMiCAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Beyza Ermis</a>, <a href="https://marziehf.github.io/" rel="external nofollow noopener" target="_blank">Marzieh Fadaee</a>, and <a href="https://www.sarahooker.me/" rel="external nofollow noopener" target="_blank">Sara Hooker</a>).</p> <p>These days, my current focus is on <strong>scaling reasoning compute</strong>. My most recent work shows that test-time scaling of English models can unlock crosslingual reasoning through ‚Äúquote-and-think‚Äù pattern <a href="https://arxiv.org/abs/2505.05408" rel="external nofollow noopener" target="_blank">(preprint)</a>. More work to come soon :)</p> <p>I also work on <strong>safety alignment</strong>. I was the first to discover that low-resource languages can <u>jailbreak</u> GPT-4 <a href="https://arxiv.org/abs/2310.02446" rel="external nofollow noopener" target="_blank">(‚≠ëBest Paper Award, NeurIPS 2023 Socially Responsible Language Modeling Workshop)</a>, This work pioneered multilingual red-teaming, which has since been incorporated into the safety frameworks of major AI developers including <a href="https://cdn.openai.com/gpt-4o-system-card.pdf" rel="external nofollow noopener" target="_blank">OpenAI</a>, <a href="https://arxiv.org/abs/2407.21783" rel="external nofollow noopener" target="_blank">Meta</a>, <a href="https://arxiv.org/abs/2407.13833" rel="external nofollow noopener" target="_blank">Microsoft</a>, etc. The work was also highlighted in the <a href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai" rel="external nofollow noopener" target="_blank">first International Scientific Report on the Safety of Advanced AI (2024)</a> and featured on <a href="https://www.newscientist.com/article/2398656-gpt-4-gave-advice-on-planning-terrorist-attacks-when-asked-in-zulu/" rel="external nofollow noopener" target="_blank">New Scientist</a>. Besides, I have worked on <u>interpretability study</u> to explain crosslingual transfer of alignment training <a href="https://arxiv.org/abs/2406.16235" rel="external nofollow noopener" target="_blank">(EMNLP 2024 Findings)</a> and fine-tuning attacks <a href="https://arxiv.org/abs/2410.18210" rel="external nofollow noopener" target="_blank">(NAACL 2025 Findings)</a>. I also have industry experience <u>red-teaming</u> frontier LLMs such as <a href="https://cohere.com/research/aya" rel="external nofollow noopener" target="_blank">Aya model (Cohere Labs)</a>.</p> <p>Previously, I worked on <strong>creating multilingual LLMs</strong>. I was the co-first author of Aya models <a href="https://arxiv.org/abs/2402.07827" rel="external nofollow noopener" target="_blank">(‚≠ëBest Paper Award, ACL 2024)</a>. I also worked on <u>speech technology</u>, particularly on understanding accent bias in ASR (INTERSPEECH 2025). A major component of my early research was on <u>low-resource NLP</u>, such as language adaptation <a href="https://arxiv.org/abs/2212.09535" rel="external nofollow noopener" target="_blank">(ACL 2023)</a>, synthetic data generation <a href="https://arxiv.org/abs/2402.14086" rel="external nofollow noopener" target="_blank">(EMNLP 2024 Findings)</a>, and code-switching (<a href="https://arxiv.org/abs/2303.13592" rel="external nofollow noopener" target="_blank">EMNLP 2023 CALCS</a>, <a href="https://aclanthology.org/2023.findings-emnlp.382/" rel="external nofollow noopener" target="_blank">EMNLP 2023 Findings</a>, <a href="https://aclanthology.org/2023.findings-acl.185/" rel="external nofollow noopener" target="_blank">ACL 2023 Findings</a>). I was also a major contributor to NLP for SEA languages (<a href="https://arxiv.org/abs/2406.10118" rel="external nofollow noopener" target="_blank">EMNLP 2024 (SEACrowd)</a>, <a href="https://arxiv.org/abs/2406.05967" rel="external nofollow noopener" target="_blank">NeurIPS 2024 (CVQA)</a>, <a href="https://aclanthology.org/2023.ijcnlp-tutorials.2/" rel="external nofollow noopener" target="_blank">*ACL 2023 tutorial</a>).</p> <p>Other Misc Stuff:</p> <ul> <li>I went to <a href="https://www.minerva.edu/" rel="external nofollow noopener" target="_blank">Minerva University</a> for my undergrad so I had the opportunity to travel and live in six different cities (for at least 4 months in each city) around the world: üá∫üá∏ San Francisco, üá∞üá∑ Seoul, üáÆüá≥ Hyderabad, üá©üá™ Berlin, üá¶üá∑ Buenos Aires and üá¨üáß London.</li> <li>My passion hobby is dancing üï∫, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. <br>I usually check out the dance scenes in the city when I travel to conferences ‚Äì‚Äì‚Äì if you also enjoy dancing, hmu we can check them out together.</li> </ul> </div> <hr> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2025</span></th> <td> <span style="color:black"><strong>1 paper accepted!</strong> Work on mitigating accent bias in ASR was accepted to INTERSPEECH‚Äô25. <br>Work was done during Meta internship. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">02 / 2025</span></th> <td> <span style="color:black"><strong>1 paper accepted!</strong> Work on <a href="https://arxiv.org/abs/2410.18210" rel="external nofollow noopener" target="_blank">cross-lingual finetuning attacks</a> was accepted to NAACL‚Äô25 findings.<br> Work was done during Meta internship. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">09 / 2024</span></th> <td> <span style="color:black"><strong>4 papers accepted!</strong> <a href="https://arxiv.org/abs/2402.14086" rel="external nofollow noopener" target="_blank">LexC-Gen</a>, <a href="https://arxiv.org/abs/2406.10118" rel="external nofollow noopener" target="_blank">SEACrowd</a>, and <a href="https://arxiv.org/abs/2406.16235" rel="external nofollow noopener" target="_blank">crosslingual alignment</a> were accepted to EMNLP. <a href="https://arxiv.org/abs/2406.05967" rel="external nofollow noopener" target="_blank">CVQA</a> was accepted to NeurIPS. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">08 / 2024</span></th> <td> <span style="color:black"><strong><a href="https://arxiv.org/abs/2402.07827" rel="external nofollow noopener" target="_blank"><span style="color:brown;">Aya Model paper</span></a></strong> received the <strong>‚≠ëBest Paper Award</strong> at ACL 2024. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">07 / 2024</span></th> <td> <span style="color:black">Gave an invited talk at <a href="https://lu.ma/fl2t9xms" rel="external nofollow noopener" target="_blank">London Data Week</a>. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">06 / 2024</span></th> <td> <span style="color:black">Started research scientist internship at Meta AI (FAIR)! </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2024</span></th> <td> <span style="color:black"><strong>1 paper accepted!</strong> <a href="https://arxiv.org/abs/2403.04893" rel="external nofollow noopener" target="_blank">A Safe Harbor for AI Evaluation and Red Teaming</a> is accepted to ICML. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">02 / 2024</span></th> <td> <span style="color:black">Released <a href="https://cohere.com/research/aya" rel="external nofollow noopener" target="_blank">Aya</a> model and dataset papers! <br>I also presented Aya multilingual safety research at Aya Grand Finale. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">11 / 2023</span></th> <td> <span style="color:black">Co-organized the <a href="https://aacl2023-sea-nlp.github.io/" rel="external nofollow noopener" target="_blank">tutorial on current status of NLP in South East Asia</a> at AACL 2023. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">10 / 2023</span></th> <td> <span style="color:black">‚Äú<strong><a href="https://arxiv.org/abs/2310.02446" rel="external nofollow noopener" target="_blank"><span style="color:brown;">Low-Resource Languages Jailbreak GPT-4</span></a></strong>‚Äù received the <strong>‚≠ëBest Paper Award</strong> at NeurIPS 2023 Socially Responsible Language Modeling (SoLaR) workshop. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">09 / 2023</span></th> <td> <span style="color:black">Joined the Cohere For AI‚Äôs Responsible Deployment Team for <a href="https://sites.google.com/cohere.com/aya-en/home" rel="external nofollow noopener" target="_blank">Aya</a> red-teaming. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">08 / 2023</span></th> <td> <span style="color:black">Served as the Area Chair (Multilingualism &amp; Linguistic Diversity Track in <a href="https://2023.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2023</a>). </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2023</span></th> <td> <span style="color:black"><strong>Media</strong>: Our <a href="https://arxiv.org/abs/2303.13592" rel="external nofollow noopener" target="_blank">code-switching paper</a> was featured by <a href="https://www.wired.com/story/chatgpt-non-english-languages-ai-revolution/" rel="external nofollow noopener" target="_blank">Wired</a>. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">05 / 2023</span></th> <td> <span style="color:black"><strong>3 papers accepted!</strong> <a href="https://arxiv.org/abs/2212.09535" rel="external nofollow noopener" target="_blank">BLOOM+1</a>, <a href="https://arxiv.org/abs/2211.01786" rel="external nofollow noopener" target="_blank">BLOOMZ</a> and <a href="https://arxiv.org/abs/2212.09660" rel="external nofollow noopener" target="_blank">code-switching survey</a> were accepted to ACL 2023. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">03 / 2022</span></th> <td> <span style="color:black"><strong>2 papers accepted!</strong> <a href="https://arxiv.org/abs/2110.08207" rel="external nofollow noopener" target="_blank">T0</a> was accepted to ICLR (Spotlight). <a href="https://arxiv.org/abs/2202.01279" rel="external nofollow noopener" target="_blank">PromptSource</a> was accepted to ACL Demo. </span> </td> </tr> <tr> <th scope="row" style="width: 20%"><span style="color:black">06 / 2021</span></th> <td> <span style="color:black">Started PhD at Brown University. </span> </td> </tr> </table> </div> </div> <hr> <h2> selected publications (<a href="/publications/" style="color: inherit">see all</a>) </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="yong2025crosslingualtesttimescaling" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2505.05408" target="_blank" style="color: black;" rel="external nofollow noopener">Crosslingual Reasoning through Test-Time Scaling</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,¬† M. Farid Adilazuarda ,¬† Jonibek Mansurov , and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen H. Bach, Alham Fikri Aji' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>arxiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2505.05408" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/BatsResearch/crosslingual-test-time-scaling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM‚Äôs CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="li2024preference" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2406.16235" target="_blank" style="color: black;" rel="external nofollow noopener">Preference Tuning for Toxicity Mitigation Generalizes Across Languages</a> </div> <div class="author"> Xiaochen Li* ,¬† <span style="font-weight:bold">Zheng-Xin Yong*</span> ,¬† and¬† Stephen H Bach </div> <div class="periodical"> <em>EMNLP Findings</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2406.16235" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/BatsResearch/cross-lingual-detox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Detoxifying multilingual Large Language Models (LLMs) has become crucial due to their increasing global use. In this work, we explore zero-shot cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike previous studies that show limited cross-lingual generalization for other safety tasks, we demonstrate that Direct Preference Optimization (DPO) training with only English data can significantly reduce toxicity in multilingual open-ended generations. For example, the probability of mGPT-1.3B generating toxic continuations drops from 46.8% to 3.9% across 17 different languages after training. Our results also extend to other multilingual LLMs, such as BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal intervention and activation analysis, we identified the dual multilinguality property of MLP layers in LLMs, which explains the cross-lingual generalization of DPO. Finally, we show that bilingual sentence retrieval can predict the cross-lingual transferability of DPO preference tuning.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="ustun2024aya" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2402.07827" target="_blank" style="color: black;" rel="external nofollow noopener">Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model</a> </div> <div class="author"> Ahmet √úst√ºn* ,¬† Viraat Aryabumi* ,¬† <span style="font-weight:bold">Zheng-Xin Yong*</span> , and <span class="more-authors" title="click to view 14 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '14 more authors' ? 'Wei-Yin Ko*, Daniel D‚Äôsouza*, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker' : '14 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">14 more authors</span> </div> <div class="periodical"> <em>ACL</em>, 2024 <span style="color:brown; font-weight:bold"> (Best Paper Award)</span> </div> <div class="periodical"> </div> <div> <span style="background-color: #FFEBB3;">Also featured in:</span> <a href="https://www.washingtonpost.com/politics/2024/02/13/ftcs-bedoya-says-laws-keep-teens-off-social-media-wont-work" rel="external nofollow noopener" target="_blank">The Washington Post</a> , <a href="https://www.theglobeandmail.com/business/article-ai-chatbots-fall-short-in-dozens-of-languages-a-non-profit-project" rel="external nofollow noopener" target="_blank">The Globe and Mail</a> , <a href="https://siliconangle.com/2024/02/13/cohere-ai-unveils-aya-multilingual-open-source-ai-101-languages/" rel="external nofollow noopener" target="_blank">SiliconANGLE</a>, etc. </div> <div class="links"> <a href="https://arxiv.org/abs/2402.07827" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages ‚Äì including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong2023lowresource" class="col-sm-11"> <div class="title"> <a href="https://arxiv.org/abs/2310.02446" target="_blank" style="color: black;" rel="external nofollow noopener">Low-Resource Languages Jailbreak GPT-4</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,¬† Cristina Menghini ,¬† and¬† Stephen Bach </div> <div class="periodical"> <em>NeurIPS Workshop: Socially Responsible Language Modelling Research (SoLaR)</em> , 2023 <span style="color:brown; font-weight:bold"> (Best Paper Award)</span> </div> <div class="periodical"> </div> <div> <span style="background-color: #FFEBB3;">Also featured in:</span> <a href="https://www.newscientist.com/article/2398656-gpt-4-gave-advice-on-planning-terrorist-attacks-when-asked-in-zulu/" rel="external nofollow noopener" target="_blank">New Scientist</a> , <a href="https://hal.science/hal-04612963/document" rel="external nofollow noopener" target="_blank">UK AI Safety Institute (Scientific Report)</a> , <a href="https://www.zdnet.com/article/the-safety-of-openais-gpt-4-is-lost-in-translation/" rel="external nofollow noopener" target="_blank">ZDNet</a>, etc. </div> <div class="links"> <a href="https://arxiv.org/abs/2310.02446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> </div> <div class="abstract hidden"> <p>AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4‚Äôs safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs‚Äô safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yong-etal-2023-bloom" class="col-sm-11"> <div class="title"> <a href="https://aclanthology.org/2023.acl-long.653.pdf" target="_blank" style="color: black;" rel="external nofollow noopener">BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting</a> </div> <div class="author"> <span style="font-weight:bold">Zheng-Xin Yong</span> ,¬† Hailey Schoelkopf ,¬† Niklas Muennighoff , and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, Vassilina Nikoulina' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">12 more authors</span> </div> <div class="periodical"> <em>ACL</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://aclanthology.org/2023.acl-long.653.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/bigscience-workshop/multilingual-modeling" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at \urlhttps://github.com/bigscience-workshop/multilingual-modeling.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="bach-etal-2022-promptsource" class="col-sm-11"> <div class="title"> <a href="https://aclanthology.org/2022.acl-demo.9.pdf" target="_blank" style="color: black;" rel="external nofollow noopener">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</a> </div> <div class="author"> Stephen Bach* ,¬† Victor Sanh* ,¬† <span style="font-weight:bold">Zheng-Xin Yong</span> , and <span class="more-authors" title="click to view 24 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '24 more authors' ? 'Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, Alexander Rush' : '24 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">24 more authors</span> </div> <div class="periodical"> <em>ACL Demo</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://aclanthology.org/2022.acl-demo.9.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://github.com/bigscience-workshop/promptsource" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at \urlhttps://github.com/bigscience-workshop/promptsource.</p> </div> </div> </div> </li> </ol> </div> <hr> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%6F%6E%74%61%63%74.%79%6F%6E%67@%62%72%6F%77%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=z6bOk-AAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/yong_zhengxin" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">Most active on X (Twitter). Also feel free to email me. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Yong Zheng-Xin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Inspired by <a href="https://akariasai.github.io/" target="_blank" rel="external nofollow noopener">Akari Asai</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-K54RERLKHP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-K54RERLKHP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>