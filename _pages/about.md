---
layout: about
title: about
permalink: /
subtitle: <span style='color:grey'>Current&#58; Computer Science Ph.D. @ Brown University<br>Past&#58; Research Scientist Intern @ <a href='https://ai.meta.com/' style='color:#222222'>Meta AI</a>, Research Collaborator @ <a href='https://cohere.com/research' style='color:#222222'>Cohere Labs</a></span>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: true # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

<style type="text/css">
 .tab { margin-left: 30px; }
</style>


I am a fourth-year Ph.D. student in Computer Science at Brown University, advised by [Prof. Stephen Bach](https://scholar.google.com/citations?user=hs6pGXoAAAAJ&hl=en). I investigate fundamental properties of large language models, particularly how they acquire and transfer capabilities across languages, domains, and tasks. I am fortunate to have worked with researchers at <u>Meta GenAI</u> (with [Jianfeng Chi](https://jfchi.github.io/)), <u>Meta AI FAIR</u> (with [Jean Maillard](https://maillard.it/) and [Michael Auli](https://michaelauli.github.io/)), and <u>Cohere Labs</u> (with [Julia Kreutzer](https://juliakreutzer.github.io/), [Beyza Ermis](https://scholar.google.com/citations?user=v2cMiCAAAAAJ&hl=en), [Marzieh Fadaee](https://marziehf.github.io/), and [Sara Hooker](https://www.sarahooker.me/)). 

I work on generalization of post-training to make models more capable and safe, often using cross-language phenomena as scientific lens to reveal core principles of how LLMs generalize to out-of-distribution settings. I also spend time work on multilingual foundational models and thinking about their social outcomes. My research interests and past work include:
- **Generalization of reasoning and test-time thinking.** My most recent work on test-time scaling reveals vastly opposite generalization behaviors of reasoning models for cross-language and cross-domain transfers. We observed robust crosslingual generalization of English reasoning finetuning through the "quote-and-think" pattern, while finding limited cross-domain transfer from math domains [(preprint)](https://arxiv.org/abs/2505.05408). 

- **Generalization of safety alignment.** I discovered low-resource languages can jailbreak GPT-4 [(&#11089;Best Paper Award, NeurIPS 2023 Socially Responsible Language Modeling Workshop)](https://arxiv.org/abs/2310.02446), revealing core limitations in how alignment training interacts with multilingual representations. This work became the seminal work for multilingual red-teaming and has shaped safety frameworks at major AI developers including [OpenAI](https://cdn.openai.com/gpt-4o-system-card.pdf), [Meta](https://arxiv.org/abs/2407.21783), and [Microsoft](https://arxiv.org/abs/2407.13833). The work was also highlighted in the [first International Scientific Report on the Safety of Advanced AI (2024)](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai) and featured on [New Scientist](https://www.newscientist.com/article/2398656-gpt-4-gave-advice-on-planning-terrorist-attacks-when-asked-in-zulu/). I have since worked on the mechanistic interpretability of crosslingual detoxification [(EMNLP 2024 Findings)](https://arxiv.org/abs/2406.16235) and crosslingual finetuning attacks [(NAACL 2025 Findings)](https://arxiv.org/abs/2410.18210).

- **Generalization of instruction-following.** I was the co-first author of the open-source Aya model [(&#11089;Best Paper Award, ACL 2024)](https://arxiv.org/abs/2402.07827) and a core contributor to foundational instruction-following models T0 ([ICLR 2022 Spotlight](https://arxiv.org/abs/2110.08207), [ACL 2022 Demo](https://arxiv.org/abs/2202.01279)) and mT0 [(ACL 2023)](https://arxiv.org/abs/2110.08207). These work demonstrated that models can learn generalizable instruction-following patterns that transfer across linguistic boundaries.

- **Efficient adaptation to unseen languages and speech accents**: I investigate how pretrained models can efficiently generalize to unseen languages through language adaptation ([ACL 2023](https://arxiv.org/abs/2212.09535)) and synthetic data [(EMNLP 2024 Findings)](https://arxiv.org/abs/2402.14086), contributing to mid-training methodologies. I also studied speech pattern distributions across accent groups to understand generalization principles to unseen accents in automatic speech recognition (INTERSPEECH 2025).

Other Misc Stuff:
- I am a Malaysian ðŸ‡²ðŸ‡¾ and I contributed substantially to NLP for South-East Asian (SEA) languages. For instance, I studied language-mixing behaviors for SEA languages ([EMNLP 2023 CALCS](https://arxiv.org/abs/2303.13592), [EMNLP 2023 Findings](https://aclanthology.org/2023.findings-emnlp.382/), [ACL 2023 Findings](https://aclanthology.org/2023.findings-acl.185/)), contributed Malaysian cultural data to [SEACrowd](https://arxiv.org/abs/2406.10118) and [CVQA](https://arxiv.org/abs/2406.05967), and co-hosted a tutorial for SEA NLP ([AACL 2023](https://aclanthology.org/2023.ijcnlp-tutorials.2/)).
- I went to [Minerva University](https://www.minerva.edu/) during my undergrad so I had the opportunity to travel and live in six different countries for at least four months: United States (San Francisco), South Korea (Seoul), India (Hyderabad), Germany (Berlin), Argentina (Buenos Aires), and United Kingdom (London).
- One of my biggest passion outside of work is dancing ðŸ•º, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. I usually check out the dance scenes in the city when I travel to conferences â€“â€“â€“ if you also enjoy dancing, hmu we can check them out together.
