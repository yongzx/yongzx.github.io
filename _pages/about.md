---
layout: about
title: about
permalink: /
subtitle: <span style='color:grey'>Current&#58; Computer Science Ph.D. @ Brown University<br>Past&#58; Research Scientist Intern @ <a href='https://ai.meta.com/' style='color:#222222'>Meta AI</a>, Research Collaborator @ <a href='https://cohere.com/research' style='color:#222222'>Cohere For AI</a></span>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: true # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a fourth-year Ph.D. student in Computer Science at Brown University, advised by [Prof. Stephen Bach](https://scholar.google.com/citations?user=hs6pGXoAAAAJ&hl=en). I am also fortunate to have done research at [Meta (FAIR and GenAI Safety Alignment)](https://ai.meta.com/research/), [Cohere For AI](https://cohere.com/research/aya), and [BigScience](https://bigscience.huggingface.co/). My broad research vision is to develop LLMs that are helpful and safe for everyone in the world.

My current focus is on **scaling reasoning compute**. My most recent work shows that [test-time scaling of English-centric models improves crosslingual reasoning as they quote-and-think](https://arxiv.org/abs/2505.05408). More work to come soon :)

I also work on **safety alignment**. I was the first to discover that low-resource languages can jailbreak GPT-4 [(&#11089;Best Paper Award, NeurIPS 2023 Socially Responsible Language Modeling Workshop)](https://arxiv.org/abs/2310.02446), This work pioneered multilingual red-teaming and was highlighted in the [International Scientific Report on the Safety of Advanced AI 2024](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai). I also study *mechanisms* of crosslingual generalization for safety alignment training [(EMNLP 2024 Findings)](https://arxiv.org/abs/2406.16235) and fine-tuning attacks [(NAACL 2025 Findings)](https://arxiv.org/abs/2410.18210). In addition, I have gained industry experience red-teaming for frontier multilingual LLMs such as [Aya model (Cohere Labs)](https://cohere.com/research/aya).

I contributed to developing **frontier open-sourced multilingual LLMs**, such as Aya models [(&#11089;Best Paper Award, ACL 2024)](https://arxiv.org/abs/2402.07827) and [mT0/BLOOMZ (ACL 2023)](https://aclanthology.org/2023.acl-long.891.pdf). I also worked on NLP for low-resource languages, such as language adaptation [(ACL 2023)](https://arxiv.org/abs/2212.09535), synthetic data generation [(EMNLP 2024 Findings)](https://arxiv.org/abs/2402.14086), and accent bias mitigation (to appear). As a Malaysian, I also contributed to NLP for Southeast Asian (SEA) languages. I've co-hosted [*ACL tutorial (2023)](https://aclanthology.org/2023.ijcnlp-tutorials.2/), helped curate SEACrowd data hub [(EMNLP 2024)](https://arxiv.org/abs/2406.10118), and studied how well LLMs can handle SEA linguistic phenomenon, such as code-switching [(EMNLP 2023 CALCS Workshop)](https://arxiv.org/abs/2303.13592), and understand culture in SEA region [(NeurIPS 2024)](https://arxiv.org/abs/2406.05967).

Other Misc Stuff:
- I went to [Minerva University](https://www.minerva.edu/) for my undergrad so I had the opportunity to travel and live in six different cities (for at least 4 months in each city) around the world: ðŸ‡ºðŸ‡¸ San Francisco, ðŸ‡°ðŸ‡· Seoul, ðŸ‡®ðŸ‡³ Hyderabad, ðŸ‡©ðŸ‡ª Berlin, ðŸ‡¦ðŸ‡· Buenos Aires and ðŸ‡¬ðŸ‡§ London. 
- My passion hobby is dancing ðŸ•º, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. <br>I usually check out the dance scenes in the city when I travel to conferences â€“â€“â€“ if you also enjoy dancing, hmu we can check them out together.
