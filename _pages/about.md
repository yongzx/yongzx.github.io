---
layout: about
title: about
permalink: /
subtitle: Computer Science Ph.D. student @ <a href='https://cs.brown.edu/'>Brown University</a><br>Research Scientist Intern @ <a href='https://ai.meta.com/'>Meta AI (FAIR)</a>


profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Still in progress.

<!-- I am a rising fourth-year Ph.D. student in Computer Science at Brown University. I am fortunate to be advised by [Prof. Stephen Bach](https://cs.brown.edu/people/sbach/). My research interests are centered around **multilingual, inclusive, and safe foundational models**. I am a major contributor to open-source multilingual LLMs initiative, such as [BigScience](https://bigscience.huggingface.co/) and [Aya](https://cohere.com/blog/aya-multilingual), as well as NLP development for Southeast-Asian languages (e.g., [SEACrowd](https://arxiv.org/abs/2406.10118)).

Currently, I am a research scientist intern at [Meta AI (FAIR)](https://ai.meta.com/research/) researching multilingual/multimodal biases under the supervision of [Jean Maillard](https://maillard.it/). Prior to joining Ph.D., I obtained a B.Sc. in Computer Science from [Minerva University](https://www.minerva.edu/), and I worked with linguists [Prof. Tiago Torrent](https://www.tiagotorrent.com/), [Prof. Oliver Czulo](https://home.uni-leipzig.de/czulo/), and [Collin F. Baker](https://www.icsi.berkeley.edu/icsi/people/collinb) on frame semantics during my undergrad.

I believe that we should develop intelligent systems that can serve *everyone across the world* in a responsible way. To this end, I work on following research directions:
1. **<span style="color:brown">making frontier AI safer for all language users</span>** – My [discovery that low-resource languages can jailbreak GPT-4](https://arxiv.org/abs/2310.02446) pioneered and raised awareness for multilingual safety research (our work was featured in various media including [New Scientist](https://www.newscientist.com/article/2398656-gpt-4-gave-advice-on-planning-terrorist-attacks-when-asked-in-zulu/)). Since then, I contribute heavily to the safety research of *massively-multilingual foundational models*. I was part of the Responsible AI team for [Aya](https://arxiv.org/abs/2402.07827) at Cohere For AI, and I am currently working on bias analysis for [Massively Multilingual Speech (MMS)](https://arxiv.org/abs/2305.13516) at Meta AI. My most recent work shows that [detoxifying LLMs with DPO can generalize cross-lingually in a zero-shot fashion](https://arxiv.org/abs/2406.16235).
2. **<span style="color:brown">building open-source multilingual LMs</span>** – I've contributed to the earliest English and multilingual instruction-following LLMs, such as [T0](https://arxiv.org/abs/2110.08207) (with [PromptSource](https://arxiv.org/abs/2202.01279)) and [mT0/BLOOMZ](https://arxiv.org/abs/2211.01786). I was also involved in the [development](https://arxiv.org/abs/2211.05100) and [evaluation](https://arxiv.org/abs/2210.15424) of BLOOM model at [BigScience](https://bigscience.huggingface.co/), and I helped collect Malay instruction-following data (as [language ambassador](https://cohere.com/research/aya-contributors-test)) and perform [safety evaluation](https://arxiv.org/abs/2402.07827) for Aya model.
3. **<span style="color:brown">enabling AI to serve underrepresented populations</span>** – Frontier AI systems these days only cater to a small subset of language, leaving many communities across the world left unsupported. To overcome technological language barriers and support low-resource NLP, I led language adaption research at BigScience to [efficiently adapt BLOOM to languages unseen during training](https://arxiv.org/abs/2212.09535) and [designed synthetic data generation method for extremely low-resource languages](https://arxiv.org/abs/2402.14086). I also worked on developing NLP systems for South-East-Asian (SEA) languages, which is underrepresented in Global-North-centric NLP systems. We curated [SEACrowd benchmark](https://arxiv.org/abs/2406.10118) and showed [how poorly multilingual LLMs code-switch in SEA languages](https://arxiv.org/abs/2303.13592). -->