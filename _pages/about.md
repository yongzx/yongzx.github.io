---
layout: about
title: about
permalink: /
subtitle: <span style='color:grey'>Computer Science Ph.D. student @ Brown University<br>Research Scientist Intern @ <a href='https://ai.meta.com/' style='color:#222222'>Meta AI (FAIR)</a>, Collaborator @ <a href='https://cohere.com/research' style='color:#222222'>Cohere For AI</a></span>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a incoming fourth-year Ph.D. student in Computer Science at Brown University, advised by [Prof. Stephen Bach](https://scholar.google.com/citations?user=hs6pGXoAAAAJ&hl=en). I'm fortunate to have collaborated with amazing researchers at [Cohere For AI](https://cohere.com/research) and at [Meta AI (FAIR and GenAI Team)](https://ai.meta.com/). **Currently interning at Meta AI and will be back to Brown in Spring 2025.** 

**Lately, I focus on making multilingual LLMs safe**, especially after I discovered that [<span style="color:brown; font-weight:700;">low-resource languages can jailbreak GPT-4</span> (&#11089;Best Paper Award, NeurIPS 2023 Socially Responsible Language Modeling Workshop)](https://arxiv.org/abs/2310.02446). This work was highlighted in the [International Scientific Report on the Safety of Advanced AI 2024](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai), and it catalyzed the paradigm shift in industry AI labs towards multilingual red-teaming.

My other notable contributions in AI safety includes:
- üîç I study why safety alignment works or fails in multilingual context (often with mechanistic interpretability). For instance, why toxicity reduction can generalize across languages [(Findings of EMNLP 2024)](https://arxiv.org/abs/2406.16235) and why monolingual finetuning attacks can undo multilingual safety guardrails ([preprint](https://arxiv.org/abs/2410.18210)).
- üåê I think about how the global open science community can contribute to AI safety research. For instance, I joined the advocate for [A Safe Harbor for AI Evaluation and Red Teaming (ICML 2024)](https://arxiv.org/abs/2403.04893) (with an [&#x1F4C4; open letter](https://sites.mit.edu/ai-safe-harbor/) signed by 300+ researchers and covered by [The Washington Post](https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney/) and [VentureBeat](https://venturebeat.com/ai/experts-call-for-legal-safe-harbor-so-researchers-journalists-and-artists-can-evaluate-ai-tools/)) to ensure legal and technical protections for AI red-teaming by independent researchers.
- üîì I have done LLM safety red-teaming research in industry AI labs. 
  - **<span style="color:brown;">Cohere For AI</span>**: I worked on red-teaming Aya-101 [(&#11089;Best Paper Award, ACL 2024)](https://arxiv.org/abs/2402.07827). 
  - **<span style="color:brown;">Meta GenAI Trust (formerly Responsible AI)</span>**: I mentored on a project of understanding finetuning attacks on multilingual LLMs such as Llama-3.1 and QWen-2 ([preprint](https://arxiv.org/abs/2410.18210)).

**I also worked on making foundational models overcome language barriers** so AI can serve all users around the world, including those who speak underrepresented languages. I have worked on both model-centric and data-centric solutions. I've worked on adapting LLMs to low-resource languagesn [(ACL 2023)](https://arxiv.org/abs/2212.09535) and generating synthetic data for low-resource languages [(Findings of EMNLP 2024)](https://arxiv.org/abs/2402.14086). I have also worked on massively multilingual LLMs and speech technology at following labs/groups:
- **Meta Fundamental AI Research (FAIR)**: I worked on mitigating accent bias for the [Massively Multilingual Speech model](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/).
- **Cohere For AI**: I served as a Malay language co-ambassador who coordinated the data collection efforts for Malay language in [Aya dataset](https://arxiv.org/abs/2402.06619). 
- **BigScience**: I led language-modeling group to [adapt BLOOM to unseen languages](https://arxiv.org/abs/2212.09535). I also contributed to [T0](https://arxiv.org/abs/2110.08207) (one of the earliest instruction-following LLM), [BLOOM](https://arxiv.org/abs/2211.05100) (the world's first largest open multilingual LLMs), and [mT0/BLOOMZ](https://arxiv.org/abs/2211.01786) (the world's first instruction-following multilingual LLM).
  
**As a Malaysian, I also contributed to NLP for Southeast Asian (SEA) languages**. I've hosted [*ACL tutorials](https://aclanthology.org/2023.ijcnlp-tutorials.2/), helped curate SEACrowd data hub [(EMNLP 2024)](https://arxiv.org/abs/2406.10118), and studied how well LLMs can handle SEA linguistic phenomenon, such as code-switching [(EMNLP 2023 CALCS Workshop)](https://arxiv.org/abs/2303.13592), and understand culture in SEA region [(NeurIPS 2024)](https://arxiv.org/abs/2406.05967).

**Other Misc Stuff:**
- If you want to chat or collaborate on any of the research directions above (or just talk about graduate schools), feel free to send an email to me: `contact [dot] yong @ brown [dot] edu`.
- My passion hobby is dancing, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. <br>I usually check out the dance scenes in the city when I travel to conferences ‚Äì‚Äì‚Äì if you also enjoy dancing, hmu we can check them out together.
- I went to [Minerva University](https://www.minerva.edu/) during undergrad so I had the opportunity to travel and live in six different cities around the world: üá∫üá∏ San Francisco, üá∞üá∑ Seoul, üáÆüá≥ Hyderabad, üá©üá™ Berlin, üá¶üá∑ Buenos Aires and üá¨üáß London. 