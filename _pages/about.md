---
layout: about
title: about
permalink: /
subtitle: <span style='color:grey'>Computer Science Ph.D. student @ Brown University<br>Research Scientist Intern @ <a href='https://ai.meta.com/' style='color:#222222'>Meta AI (FAIR)</a>, Collaborator @ <a href='https://cohere.com/research' style='color:#222222'>Cohere For AI</a></span>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a incoming fourth-year Ph.D. student in Computer Science at Brown University, advised by [Prof. Stephen Bach](https://scholar.google.com/citations?user=hs6pGXoAAAAJ&hl=en). I'm fortunate to have collaborated with amazing researchers at [Cohere For AI](https://cohere.com/research) and at [Meta AI (FAIR and GenAI Trust Team)](https://ai.meta.com/). My long term goal is to develop safe and inclusive general-purpose AI. **Currently interning at Meta AI and will be back to Brown in Spring 2025.** 

**Lately, I focus on making multilingual LLMs safer.** I research cross-lingual transfer of safety knowledge, such as toxicity [(Findings of EMNLP 2024)](https://arxiv.org/abs/2406.16235) and harmful-finetuning (coming soon). My other notable work in the AI safety includes:
- I discovered cross-lingual vulnerability in LLM safety guardrails as I found ["Low-Resource Languages Jailbreak GPT-4" (NeurIPS 2023 Socially Responsible Language Modeling Workshop, &#11089;Best Paper Award)](https://arxiv.org/abs/2310.02446). This discovery catalyzed the paradigm shift towards multilingual red-teaming and was highlighted by the UK Government and AI Safety Institute in the [International Scientific Report on the Safety of Advanced AI 2024](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai).
- I collaborated with Cohere For AI to perform multilingual safety red-teaming for Aya-101 [(ACL 2024, &#11089;Best Paper Award)](https://arxiv.org/abs/2402.07827), which is the state-of-the-art open LLM that follows instructions in 101 languages.

**I also work on making foundational models overcome language barriers** so AI can serve all users around the world, including those who speak underrepresented languages. I have worked on both model-centric and data-centric solutions.
- I researched how to finetune multilingual LLMs, such as BLOOM+1 [(ACL 2023)](https://arxiv.org/abs/2212.09535), to adapt to low-resource languages unseen during pretraining.
- I proposed novel methods to generate training data for low-resource languages. For the very first time, synthetic labeled data generated by my proposed LexC-Gen [(Findings of EMNLP 2024)](https://arxiv.org/abs/2402.14086) can match the performance of manually collected training data for very low-resource languages in Indonesia and Africa.
- I helped build *massively multilingual* LLMs and speech technology:
  - **[<span style="color:brown;">Meta AI (FAIR)</span>](https://ai.meta.com/research/)**: I worked on mitigating accent bias for the [Massively Multilingual Speech model](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/).
  - **[<span style="color:brown;">Cohere For AI</span>](https://cohere.com/research/aya)**: I served as language ambassador who coordinated the data collection efforts for Malay language in [Aya dataset](https://arxiv.org/abs/2402.06619). In addition, I was part of safety red-teaming team for [Aya models](https://arxiv.org/abs/2402.07827).
  - **[<span style="color:brown;">BigScience</span>](https://bigscience.huggingface.co/)**: I contributed to LLMs such as [BLOOM](https://arxiv.org/abs/2211.05100), [T0](https://arxiv.org/abs/2110.08207) and [mT0/BLOOMZ](https://arxiv.org/abs/2211.01786). In addition, I led the research efforts for [adapting BLOOM to unseen languages](https://arxiv.org/abs/2212.09535).
  
**As a Malaysian, I also contributed to NLP for Southeast Asian (SEA) languages**. I've hosted [*ACL tutorials](https://aclanthology.org/2023.ijcnlp-tutorials.2/), helped curate SEACrowd data hub [(EMNLP 2024)](https://arxiv.org/abs/2406.10118), and studied how well LLMs can handle SEA linguistic phenomenon, such as code-switching [(EMNLP 2023 CALCS Workshop)](https://arxiv.org/abs/2303.13592), and understand culture in SEA region [(NeurIPS 2024)](https://arxiv.org/abs/2406.05967).

**Other Misc Stuff:**
- If you want to chat or collaborate on any of the research directions above (or just talk about graduate schools), feel free to send an email to me: `contact [dot] yong @ brown [dot] edu`.
- My passion hobby is dancing, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. <br>I usually check out the dance scenes in the city when I travel to conferences ––– if you also enjoy dancing, hmu we can check them out together.
- I went to [Minerva University](https://www.minerva.edu/) during undergrad so I had the opportunity to travel and live in six different cities around the world: San Francisco, Seoul, Hyderabad, Berlin Buenos Aires and London. 