---
layout: about
title: about
permalink: /
subtitle: <span style='color:grey'>Computer Science Ph.D. student @ Brown University<br>Research Scientist Intern @ <a href='https://ai.meta.com/' style='color:#222222'>Meta AI (FAIR)</a>, Collaborator @ <a href='https://cohere.com/research' style='color:#222222'>Cohere For AI</a></span>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: 

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am a incoming fourth-year Ph.D. student in Computer Science at Brown University, advised by [Prof. Stephen Bach](https://scholar.google.com/citations?user=hs6pGXoAAAAJ&hl=en). I'm fortunate to have collaborated with amazing researchers at [Cohere For AI](https://cohere.com/research) and at [Meta AI (FAIR and GenAI Team)](https://ai.meta.com/). **Currently interning at Meta AI and will be back to Brown in Spring 2025.** 

**Lately, I focus on making multilingual LLMs safe**, especially after I discovered that [<span style="color:brown; font-weight:700;">low-resource languages can jailbreak GPT-4</span> (&#11089;Best Paper Award, NeurIPS 2023 Socially Responsible Language Modeling Workshop)](https://arxiv.org/abs/2310.02446). This work was highlighted in the [International Scientific Report on the Safety of Advanced AI 2024](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai), and it catalyzed the paradigm shift towards multilingual red-teaming in industry.

My other notable contributions in AI safety includes:
- I research how safety knowledge transfers across languages (often with mechanistic interpretability). For instance, we remove multilingual toxicity using only one language [(Findings of EMNLP 2024)](https://arxiv.org/abs/2406.16235) and explain cross-lingual generalization of finetuning attack ([preprint](https://arxiv.org/abs/2410.18210)). 
- I have performed safety red-teaming for LLMs from industry AI labs. For instance, I collaborated with Cohere For AI for red-teaming Aya-101 [(&#11089;Best Paper Award, ACL 2024)](https://arxiv.org/abs/2402.07827).
- I think about how open science community can contribute to AI safety research. For instance, I advocated for [A Safe Harbor for AI Evaluation and Red Teaming](https://arxiv.org/abs/2403.04893) (with an [&#x1F4C4; open letter](https://sites.mit.edu/ai-safe-harbor/) signed by 300+ researchers and covered by [The Washington Post](https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney/) and [VentureBeat](https://venturebeat.com/ai/experts-call-for-legal-safe-harbor-so-researchers-journalists-and-artists-can-evaluate-ai-tools/)) to ensure legal and technical protections for AI red-teaming by independent researchers.

**I also work on making foundational models overcome language barriers** so AI can serve all users around the world, including those who speak underrepresented languages. I have worked on both model-centric and data-centric solutions.
- I researched how to finetune multilingual LLMs, such as BLOOM+1 [(ACL 2023)](https://arxiv.org/abs/2212.09535), to adapt to low-resource languages unseen during pretraining.
- I proposed novel methods to generate training data for low-resource languages. For the very first time, synthetic labeled data generated by my proposed LexC-Gen [(Findings of EMNLP 2024)](https://arxiv.org/abs/2402.14086) can match the performance of manually collected training data for very low-resource languages in Indonesia and Africa.
- I helped build *massively multilingual* LLMs and speech technology:
  - **[<span style="color:brown;">Meta AI (FAIR)</span>](https://ai.meta.com/research/)**: I worked on mitigating accent bias for the [Massively Multilingual Speech model](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/).
  - **[<span style="color:brown;">Cohere For AI</span>](https://cohere.com/research/aya)**: I served as language ambassador who coordinated the data collection efforts for Malay language in [Aya dataset](https://arxiv.org/abs/2402.06619). In addition, I was part of safety red-teaming team for [Aya models](https://arxiv.org/abs/2402.07827).
  - **[<span style="color:brown;">BigScience</span>](https://bigscience.huggingface.co/)**: I contributed to LLMs such as [BLOOM](https://arxiv.org/abs/2211.05100), [T0](https://arxiv.org/abs/2110.08207) and [mT0/BLOOMZ](https://arxiv.org/abs/2211.01786). In addition, I led the research efforts for [adapting BLOOM to unseen languages](https://arxiv.org/abs/2212.09535).
  
**As a Malaysian, I also contributed to NLP for Southeast Asian (SEA) languages**. I've hosted [*ACL tutorials](https://aclanthology.org/2023.ijcnlp-tutorials.2/), helped curate SEACrowd data hub [(EMNLP 2024)](https://arxiv.org/abs/2406.10118), and studied how well LLMs can handle SEA linguistic phenomenon, such as code-switching [(EMNLP 2023 CALCS Workshop)](https://arxiv.org/abs/2303.13592), and understand culture in SEA region [(NeurIPS 2024)](https://arxiv.org/abs/2406.05967).

**Other Misc Stuff:**
- If you want to chat or collaborate on any of the research directions above (or just talk about graduate schools), feel free to send an email to me: `contact [dot] yong @ brown [dot] edu`.
- My passion hobby is dancing, especially salsa and bachata. I also dance a bit of Lindy Hop, Argentine Tango and K-pop. <br>I usually check out the dance scenes in the city when I travel to conferences â€“â€“â€“ if you also enjoy dancing, hmu we can check them out together.
- I went to [Minerva University](https://www.minerva.edu/) during undergrad so I had the opportunity to travel and live in six different cities around the world: ðŸ‡ºðŸ‡¸ San Francisco, ðŸ‡°ðŸ‡· Seoul, ðŸ‡®ðŸ‡³ Hyderabad, ðŸ‡©ðŸ‡ª Berlin, ðŸ‡¦ðŸ‡· Buenos Aires and ðŸ‡¬ðŸ‡§ London. 